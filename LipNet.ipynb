{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Installing and importing the libraries"
      ],
      "metadata": {
        "id": "0Z200yaFoVPM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "P_OKI0TznQc_"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python matplotlib imageio gdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from typing import List\n",
        "import gdown\n",
        "from matplotlib import pyplot as plt\n",
        "import imageio"
      ],
      "metadata": {
        "id": "e11dF8_MnXOC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv3D, LSTM, Dense, Dropout, Bidirectional, MaxPool3D, Activation, Reshape, SpatialDropout3D, BatchNormalization, TimeDistributed, Flatten\n",
        "from tensorflow.keras.optimizers import Adam,legacy\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n"
      ],
      "metadata": {
        "id": "8V6lnxTa54NV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List available physical GPU devices\n",
        "physical_devices = tf.config.list_physical_devices('GPU')"
      ],
      "metadata": {
        "id": "e3r2L8JYnbM4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "try:\n",
        "    # Attempt to set memory growth for the first GPU\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "-ddtCFWZnjzP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DataLoading\n"
      ],
      "metadata": {
        "id": "zV0X1ymnz9Dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the URL of the dataset containing videos of 1 speaker of GRID Dataset\n",
        "url = 'https://drive.google.com/uc?id=1YlvpDLix3S-U8fd-gqRwPcWXAXm8JwjL'\n",
        "\n",
        "# Define the output file name for the downloaded ZIP file\n",
        "output = 'data.zip'\n",
        "\n",
        "# Download the dataset from the specified URL\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Extract the contents of the ZIP file\n",
        "gdown.extractall('data.zip')"
      ],
      "metadata": {
        "id": "0DELMa27oQL0"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_video(path: str) -> List[float]:\n",
        "    \"\"\"\n",
        "    Load and preprocess a video from the specified path.\n",
        "\n",
        "    Parameters:\n",
        "        path (str): The path to the video file.\n",
        "\n",
        "    Returns:\n",
        "        List[float]: A list of preprocessed frames from the video.\"\"\"\n",
        "\n",
        "\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(path)\n",
        "\n",
        "    # Initialize an empty list to store frames\n",
        "    frames = []\n",
        "\n",
        "    # Loop through all frames in the video\n",
        "    for _ in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):\n",
        "        # Read a frame from the video\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # Convert the frame to grayscale\n",
        "        frame = tf.image.rgb_to_grayscale(frame)\n",
        "\n",
        "        # Crop the frame to the region of interest i.e. lip region\n",
        "        frames.append(frame[190:236, 80:220, :])\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()\n",
        "\n",
        "    # Calculate mean and standard deviation for normalization\n",
        "    mean = tf.math.reduce_mean(frames)\n",
        "    std = tf.math.reduce_std(tf.cast(frames, tf.float32))\n",
        "\n",
        "    # Normalize frames and cast to float32\n",
        "    return tf.cast((frames - mean), tf.float32) / std\n"
      ],
      "metadata": {
        "id": "07q4B__FtHO3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the vocabulary as a list of all possible characters we may encounter in our annotations\n",
        "vocab = [x for x in \"abcdefghijklmnopqrstuvwxyz'?!123456789 \"]"
      ],
      "metadata": {
        "id": "GNR_QaE8tTpK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# String to number mapping\n",
        "char_to_num = tf.keras.layers.StringLookup(\n",
        "    vocabulary=vocab, oov_token=\"\"\n",
        ")\n",
        "\n",
        "# Number to string mapping\n",
        "num_to_char = tf.keras.layers.StringLookup(\n",
        "    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",
        ")\n",
        "\n",
        "# Print vocabulary information\n",
        "print(\n",
        "    f\"The vocabulary is: {char_to_num.get_vocabulary()} \"\n",
        "    f\"(size ={char_to_num.vocabulary_size()})\"\n",
        ")"
      ],
      "metadata": {
        "id": "JlXQBa5qtu-u"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_num.get_vocabulary()"
      ],
      "metadata": {
        "id": "kE8beVkGueNs"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_alignments(path:str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Load alignments from a text file at the specified path.\n",
        "\n",
        "    Parameters:\n",
        "        path (str): The path to the text file containing alignments.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of phonetic tokens extracted from the alignments file.\n",
        "    \"\"\"\n",
        "    with open(path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    # Initialize an empty list to store phonetic tokens\n",
        "    tokens = []\n",
        "\n",
        "    # Iterate through each line in the file\n",
        "    for line in lines:\n",
        "        line = line.split()\n",
        "        # Check if the token is not 'sil' (silence)\n",
        "        if line[2] != 'sil':\n",
        "            tokens = [*tokens,' ',line[2]]\n",
        "\n",
        "    # Convert phonetic tokens to numerical indices using char_to_num layer, and exclude the first space token\n",
        "    return char_to_num(tf.reshape(tf.strings.unicode_split(tokens, input_encoding='UTF-8'), (-1)))[1:]\n"
      ],
      "metadata": {
        "id": "UdgRXbl6vKwf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path: str):\n",
        "    \"\"\"\n",
        "    Load data from the specified path\n",
        "\n",
        "    Parameters:\n",
        "        path (str): The path to the data file.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[float], List[str]]: A tuple containing the loaded video frames and alignments.\n",
        "    \"\"\"\n",
        "    # Convert path from bytes to string\n",
        "    path = bytes.decode(path.numpy())\n",
        "\n",
        "    # Extract file name from the path\n",
        "    file_name = path.split('/')[-1].split('.')[0]\n",
        "\n",
        "    # Construct paths for video and alignment files\n",
        "    video_path = os.path.join('data', 's1', f'{file_name}.mpg')\n",
        "    alignment_path = os.path.join('data', 'alignments', 's1', f'{file_name}.align')\n",
        "\n",
        "    # Load video frames\n",
        "    frames = load_video(video_path)\n",
        "\n",
        "    # Load alignments\n",
        "    alignments = load_alignments(alignment_path)\n",
        "\n",
        "    return frames, alignments\n"
      ],
      "metadata": {
        "id": "rbmURHzsxjL5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mappable_function(path: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    A mappable function to load data from the specified path.\n",
        "\n",
        "    Parameters:\n",
        "        path (str): The path to the data file.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list containing the loaded video frames and alignments.\n",
        "    \"\"\"\n",
        "    # Call load_data using tf.py_function\n",
        "    result = tf.py_function(load_data, [path], (tf.float32, tf.int64))\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "W9ihHNvS2iz4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualising One Example in our dataset\n"
      ],
      "metadata": {
        "id": "O8Lle_tCzmIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_path = '/content/data/s1/bbaf2n.mpg'"
      ],
      "metadata": {
        "id": "nky__zQJyp8P"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the data and unpack the frame and alignments\n",
        "frames, alignments = load_data(tf.convert_to_tensor(test_path))"
      ],
      "metadata": {
        "id": "J3s89cYazXxL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Show one instance of the lip\n",
        "plt.imshow(frames[40])"
      ],
      "metadata": {
        "id": "nHqaPhFBzaxY"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decode numerical alignments into human-readable phonetic tokens.\n",
        "tf.strings.reduce_join([bytes.decode(x) for x in num_to_char(alignments.numpy()).numpy()])"
      ],
      "metadata": {
        "id": "OOgR3xclzdmu"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Pipeline\n",
        "\n"
      ],
      "metadata": {
        "id": "zZtL_S_22uIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data files from the specified directory\n",
        "data = tf.data.Dataset.list_files('./data/s1/*.mpg')\n",
        "\n",
        "# Shuffle the dataset with a buffer size of 500\n",
        "data = data.shuffle(500, reshuffle_each_iteration=False)\n",
        "\n",
        "# Map the mappable_function to load data for each file in the dataset\n",
        "data = data.map(mappable_function)\n",
        "\n",
        "# Pad and batch the dataset with group size 2, padding frames to have shapes ([75,None,None,None]) and alignments to have shape ([40])\n",
        "data = data.padded_batch(2, padded_shapes=([75,None,None,None],[40]))\n",
        "\n",
        "# Prefetch data to improve pipeline performance\n",
        "data = data.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "#Split Data\n",
        "train = data.take(450)\n",
        "test = data.skip(450)"
      ],
      "metadata": {
        "id": "-YfzaSXfzgqk"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "id": "kEnxb13_42WV"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames, alignments = data.as_numpy_iterator().next()"
      ],
      "metadata": {
        "id": "EVmKsvWh5Jd1"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = data.as_numpy_iterator()"
      ],
      "metadata": {
        "id": "BJVhaEEf2jI1"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = sample.next(); val[0]"
      ],
      "metadata": {
        "id": "D-Kje8OL3l-1"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0:videos, 0: 1st video out of the batch,  35: return the 35th frame in the video\n",
        "plt.imshow(val[0][0][35])"
      ],
      "metadata": {
        "id": "5OO3MO3w3mzZ"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.as_numpy_iterator().next()[0][0].shape"
      ],
      "metadata": {
        "id": "VTWoiQIl4ONL"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MODEL\n"
      ],
      "metadata": {
        "id": "u8qyPmlA67VI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv3D, Activation, MaxPool3D, TimeDistributed, Flatten, Bidirectional, LSTM, Dropout, Dense\n",
        "\n",
        "# Create a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add a 3D convolutional layer with 128 filters, kernel size 3x3x3, and input shape of (75, 46, 140, 1)\n",
        "model.add(Conv3D(128, 3, input_shape=(75,46,140,1), padding='same'))\n",
        "\n",
        "# Add ReLU activation function\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Add 3D max pooling layer with pool size (1,2,2)\n",
        "model.add(MaxPool3D((1,2,2)))\n",
        "\n",
        "# Add another 3D convolutional layer with 256 filters and kernel size 3x3x3\n",
        "model.add(Conv3D(256, 3, padding='same'))\n",
        "\n",
        "# Add ReLU activation function\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Add 3D max pooling layer with pool size (1,2,2)\n",
        "model.add(MaxPool3D((1,2,2)))\n",
        "\n",
        "# Add another 3D convolutional layer with 75 filters and kernel size 3x3x3\n",
        "model.add(Conv3D(75, 3, padding='same'))\n",
        "\n",
        "# Add ReLU activation function\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Add 3D max pooling layer with pool size (1,2,2)\n",
        "model.add(MaxPool3D((1,2,2)))\n",
        "\n",
        "# Add TimeDistributed layer to apply Flatten operation to each time step independently\n",
        "model.add(TimeDistributed(Flatten()))\n",
        "\n",
        "# Add Bidirectional LSTM layer with 128 units, using Orthogonal kernel initializer, returning sequences\n",
        "model.add(Bidirectional(LSTM(128, kernel_initializer='Orthogonal', return_sequences=True)))\n",
        "\n",
        "# Add dropout layer with dropout rate of 0.5\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Add another Bidirectional LSTM layer with 128 units, using Orthogonal kernel initializer, returning sequences\n",
        "model.add(Bidirectional(LSTM(128, kernel_initializer='Orthogonal', return_sequences=True)))\n",
        "\n",
        "# Add dropout layer with dropout rate of 0.5\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Add Dense layer with number of units equal to vocabulary size + 1, using he_normal kernel initializer and softmax activation function\n",
        "model.add(Dense(char_to_num.vocabulary_size() + 1, kernel_initializer='he_normal', activation='softmax'))\n"
      ],
      "metadata": {
        "id": "xy6FZl_T4-hB"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "6_UGkBoS6AuY"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setting up Training\n"
      ],
      "metadata": {
        "id": "L5mu-qJ370dH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scheduler(epoch, lr):\n",
        "    \"\"\"\n",
        "    Learning rate scheduler function.\n",
        "\n",
        "    Args:\n",
        "    - epoch (int): The current epoch number.\n",
        "    - lr (float): The current learning rate.\n",
        "\n",
        "    Returns:\n",
        "    - float: The updated learning rate based on the epoch number.\n",
        "    \"\"\"\n",
        "\n",
        "    if epoch < 30:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.1)\n"
      ],
      "metadata": {
        "id": "I3JRrKjI6Ra4"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CTCLoss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute the Connectionist Temporal Classification (CTC) loss.\n",
        "\n",
        "    Args:\n",
        "    - y_true (tensor): True labels. Expected to have shape (batch_size, max_label_length).\n",
        "    - y_pred (tensor): Predicted logits. Expected to have shape (batch_size, max_input_length, num_classes).\n",
        "\n",
        "    Returns:\n",
        "    - tensor: CTC loss.\n",
        "    \"\"\"\n",
        "\n",
        "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
        "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
        "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
        "\n",
        "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "\n",
        "    loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "qvU1iwTA7RyU"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProduceExample(tf.keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Callback to produce examples of model predictions at the end of each epoch.\n",
        "\n",
        "    Args:\n",
        "    - dataset (tf.data.Dataset): Dataset used for evaluation.\n",
        "\n",
        "    Methods:\n",
        "    - on_epoch_end(epoch, logs=None): Called at the end of each epoch to generate and print model predictions.\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the callback.\n",
        "\n",
        "        Args:\n",
        "        - dataset (tf.data.Dataset): Dataset used for evaluation.\n",
        "        \"\"\"\n",
        "        self.dataset = dataset.as_numpy_iterator()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None) -> None:\n",
        "        \"\"\"\n",
        "        Generate and print model predictions at the end of each epoch.\n",
        "\n",
        "        Args:\n",
        "        - epoch (int): Current epoch number.\n",
        "        - logs (dict): Dictionary containing the loss value and any other metrics during training.\n",
        "        \"\"\"\n",
        "        data = self.dataset.next()\n",
        "        yhat = self.model.predict(data[0])\n",
        "        decoded = tf.keras.backend.ctc_decode(yhat, [75,75], greedy=False)[0][0].numpy()\n",
        "        for x in range(len(yhat)):\n",
        "            print('Original:', tf.strings.reduce_join(num_to_char(data[1][x])).numpy().decode('utf-8'))\n",
        "            print('Prediction:', tf.strings.reduce_join(num_to_char(decoded[x])).numpy().decode('utf-8'))\n",
        "            print('~'*100)\n"
      ],
      "metadata": {
        "id": "Pl08bEJR7TxX"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = legacy.Adam(), loss=CTCLoss)"
      ],
      "metadata": {
        "id": "IrRQ_rYP7WWA"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_callback = ModelCheckpoint(os.path.join('models','checkpoint'), monitor='loss', save_weights_only=True)\n"
      ],
      "metadata": {
        "id": "f4staPxQ7ZG6"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schedule_callback = LearningRateScheduler(scheduler)"
      ],
      "metadata": {
        "id": "o9frn-aE7b4f"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_callback = ProduceExample(test)"
      ],
      "metadata": {
        "id": "ZIp9AcUL7eTp"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train, validation_data=test, epochs=96, callbacks=[checkpoint_callback, schedule_callback, example_callback])\n"
      ],
      "metadata": {
        "id": "lXcD3wOR7wpd"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PREDICTIONS"
      ],
      "metadata": {
        "id": "kZhDa48m8vca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://drive.google.com/uc?id=1vWscXs4Vt0a_1IH1-ct2TCgXAZT-N3_Y'\n",
        "output = 'checkpoints.zip'\n",
        "gdown.download(url, output, quiet=False)\n",
        "gdown.extractall('checkpoints.zip', 'models')"
      ],
      "metadata": {
        "id": "7NMZH9IK7183"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('models/checkpoint')"
      ],
      "metadata": {
        "id": "AmJiCOgcDFsC"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tPWjmTrbDIh5"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = legacy.Adam()"
      ],
      "metadata": {
        "id": "dNtbbwSbGO7W"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = test.as_numpy_iterator()"
      ],
      "metadata": {
        "id": "N1Gh-egjGW9g"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = test_data.next()"
      ],
      "metadata": {
        "id": "L_d-m7sQG1Xt"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = model.predict(sample[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mf5eG3bgG3ic",
        "outputId": "0d0a2bbf-6ef4-422b-c261-33fbf55e1b8f"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 6s 6s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('~'*100, 'REAL TEXT')\n",
        "[tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in sample[1]]"
      ],
      "metadata": {
        "id": "ehZSQuCxG6Qm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ebf7b48-2fa2-4582-e0b9-96a39fa2b85b"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ REAL TEXT\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=string, numpy=b'lay green at z seven soon'>,\n",
              " <tf.Tensor: shape=(), dtype=string, numpy=b'place white with e zero now'>]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded = tf.keras.backend.ctc_decode(yhat, input_length=[75,75], greedy=True)[0][0].numpy()"
      ],
      "metadata": {
        "id": "HbyJx49f5Uf3"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('~'*100, 'PREDICTIONS')\n",
        "[tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in decoded]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OisAY54v5VGH",
        "outputId": "40a6cdca-6657-4c70-c162-0725df8dfda3"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ PREDICTIONS\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=string, numpy=b'lay green at z seven soon'>,\n",
              " <tf.Tensor: shape=(), dtype=string, numpy=b'place white with zero now'>]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = load_data(tf.convert_to_tensor('/content/data/s1/lrarzn.mpg'))"
      ],
      "metadata": {
        "id": "eNl1nh815bLv"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('~'*100, 'REAL TEXT')\n",
        "[tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in [sample[1]]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCSCv26y5xpJ",
        "outputId": "12af9ce1-0573-404f-a392-8d94a5ad4561"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ REAL TEXT\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=string, numpy=b'lay red at r zero now'>]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = model.predict(tf.expand_dims(sample[0], axis=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14mU2ArU57Ck",
        "outputId": "84adb61c-504d-46a1-8e90-bae2b82831d5"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 149ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded = tf.keras.backend.ctc_decode(yhat, input_length=[75], greedy=True)[0][0].numpy()"
      ],
      "metadata": {
        "id": "-URn7E0j59Ub"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('~'*100, 'PREDICTIONS')\n",
        "[tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in decoded]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIuAfbYO5_ny",
        "outputId": "8eede7db-7ed2-4aa0-9b38-c9bcdcf65e5e"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ PREDICTIONS\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=string, numpy=b'lay red at r zero now'>]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MUMcuKFX6Bb_"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XIBPcO986O8D"
      },
      "execution_count": 72,
      "outputs": []
    }
  ]
}